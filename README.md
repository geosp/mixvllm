# vLLM Development Environment

Development environment for vLLM with dual-GPU tensor parallelism support.

## Hardware

- **GPUs**: 2x NVIDIA GeForce RTX 3090 Ti (24GB VRAM each)
- **CUDA Version**: 12.8
- **Driver**: 570.172.08

## Setup

This project was initialized using `create_project.sh` at the root.

### Install Dependencies

```bash
# Install all dependencies including dev tools
uv sync --all-extras

# Or install without dev dependencies
uv sync
```

### Activate Environment

```bash
# Run commands with uv (recommended)
uv run python script.py

# Or activate the virtual environment
source .venv/bin/activate
```

## Project Structure

```
.
â”œâ”€â”€ create_project.sh     # Project bootstrap script
â”œâ”€â”€ .claude/              # Temporary and experimental code
â”‚   â”œâ”€â”€ experiments/      # Model testing and prototyping
â”‚   â”œâ”€â”€ benchmarks/       # Performance benchmarks
â”‚   â””â”€â”€ scratch/          # Quick tests and scratchpad
â”œâ”€â”€ src/                  # Production code
â”‚   â”œâ”€â”€ inference/        # vLLM inference wrappers
â”‚   â””â”€â”€ utils/            # Shared utilities
â”œâ”€â”€ configs/              # Model and server configurations
â””â”€â”€ tests/                # Test suite
```

## Quick Start

### 1. Test GPU Detection

```bash
uv run python .claude/experiments/test_gpu.py
```

### 2. Test vLLM Installation

```bash
uv run python .claude/experiments/test_vllm.py
```

### 3. Run a Model with Tensor Parallelism

```python
from vllm import LLM

llm = LLM(
    model="meta-llama/Llama-2-70b-hf",
    tensor_parallel_size=2,  # Use both GPUs
    gpu_memory_utilization=0.90,
    trust_remote_code=True
)

outputs = llm.generate("Hello, my name is")
print(outputs[0].outputs[0].text)
```

## Configuration

See `configs/example_model.yaml` for a complete configuration template.

## Development

```bash
# Run tests
uv run pytest

# Type checking
uv run mypy src/

# Linting
uv run ruff check src/

# Auto-formatting
uv run black src/
```

## Tensor Parallelism Notes

With 2x RTX 3090 Ti (24GB each = 48GB total):
- Can run 70B models in FP16 (requires ~140GB, use quantization)
- Can run 70B models in 4-bit quantization comfortably
- Can run 34B models in FP16 easily
- Communication overhead between GPUs is minimal on PCIe 4.0

## Troubleshooting

**Out of Memory Errors:**
- Reduce `gpu_memory_utilization` (try 0.85 or 0.80)
- Use quantization (4-bit or 8-bit)
- Reduce `max_model_len`

**Slow Inference:**
- Check GPU utilization with `nvidia-smi`
- Verify both GPUs are being used
- Ensure PCIe link is running at full speed

**401 Unauthorized Errors:**
- Set `HF_TOKEN` environment variable with your HuggingFace token
- For gated models, request access on the HuggingFace model page
- Verify token has read permissions: `huggingface-cli whoami`
- Some models require accepting terms/conditions on HuggingFace

## Authentication

Some models require authentication to access from HuggingFace. If you encounter `401 Unauthorized` errors, you need to:

### HuggingFace Token Setup

1. **Get a token**: Visit https://huggingface.co/settings/tokens to create an access token
2. **Set environment variable**:
   ```bash
   export HF_TOKEN=hf_xxxxxxxxxxxxxxxxxxxxxxxxx
   ```
3. **Or login via CLI**:
   ```bash
   huggingface-cli login
   ```

### Gated Models

Some models (especially from OpenAI, Meta, etc.) are **gated repositories** that require:
- âœ… Valid HuggingFace account
- âœ… Explicit access approval on the model page
- âœ… Proper authentication token

**Example with authentication:**
```bash
HF_TOKEN=$HF_TOKEN uv run mixvllm-serve --config configs/gpt-oss-20b.yaml
```

**Models that may require authentication:**
- `openai/gpt-oss-20b` (gated)
- `meta-llama/Llama-2-*` (gated)
- `meta-llama/Llama-3-*` (gated)

**Public models (no auth required):**
- `microsoft/Phi-3-mini-4k-instruct`
- Most Microsoft and Google models

## Model Serving

Serve vLLM models with the `serve_model.py` script, which provides an OpenAI-compatible API server.

### Basic Usage

```bash
# Serve Phi-3 Mini on single GPU (no auth required)
uv run mixvllm-serve --model microsoft/Phi-3-mini-4k-instruct --gpus 1

# Serve Llama 2 70B with tensor parallelism (requires HF_TOKEN)
HF_TOKEN=$HF_TOKEN uv run mixvllm-serve --model meta-llama/Llama-2-70b-hf --gpus 2 --trust-remote-code
```

### Using Configuration Files

```bash
# Use predefined configurations
uv run mixvllm-serve --config configs/phi3-mini.yaml          # No auth required
uv run mixvllm-serve --config configs/llama-7b.yaml           # May require HF_TOKEN
HF_TOKEN=$HF_TOKEN uv run mixvllm-serve --config configs/llama-70b-tp2.yaml  # Requires HF_TOKEN
HF_TOKEN=$HF_TOKEN uv run mixvllm-serve --config configs/gpt-oss-20b.yaml    # Requires HF_TOKEN

# Override config with CLI options
uv run mixvllm-serve --config configs/phi3-mini.yaml --port 8080
```

### Advanced Options

```bash
HF_TOKEN=$HF_TOKEN uv run mixvllm-serve \
  --model meta-llama/Llama-2-70b-hf \
  --gpus 2 \
  --gpu-memory 0.85 \
  --max-model-len 4096 \
  --port 8000 \
  --temperature 0.8 \
  --max-tokens 1024
```

### API Usage

Once running, the server provides an OpenAI-compatible API:

```bash
# Health check
curl http://localhost:8000/health

# Chat completion
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-3-mini-4k-instruct",
    "messages": [{"role": "user", "content": "Hello!"}]
  }'
```

## Chat Client

The `mixvllm-chat` command provides a CLI chat interface for interactive conversations with your served models. It features rich terminal formatting and enhanced input handling similar to modern CLI applications.

```bash
# Install dependencies (if not already done)
uv sync

# Start chatting with default settings
uv run mixvllm-chat

# Connect to specific server and model
uv run mixvllm-chat --base-url http://localhost:8000 --model microsoft/Phi-3-mini-4k-instruct

# Enable streaming responses
uv run mixvllm-chat --stream --temperature 0.8
```

### Chat Client Features

- **Rich Terminal UI**: Beautiful formatting with colors, panels, and markdown rendering
- **Conversation Context**: Maintains chat history for coherent conversations
- **Command Support**: `/help`, `/clear`, `/history`, `/quit`
- **Enhanced Input**: History-based auto-completion and navigation (with prompt_toolkit)
- **Streaming Support**: Real-time response streaming with live updates
- **Model Auto-detection**: Automatically detects available models from server
- **Error Handling**: Clear error messages with appropriate formatting

### Dependencies

The chat client uses these optional libraries for enhanced UI:
- `rich`: Beautiful terminal formatting and colors
- `prompt_toolkit`: Enhanced input with history and completion
- `requests`: HTTP client for API calls

If these libraries are not available, the client falls back to basic text output.

### Example Chat Session

```
âœ“ Connected to vLLM server at http://localhost:8000
âœ“ Auto-selected model: microsoft/Phi-3-mini-4k-instruct

â•­â”€ Welcome â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚                                                                            â”‚
â”‚ ğŸ¤– vLLM Chat Client                                                        â”‚
â”‚                                                                            â”‚
â”‚ Configuration:                                                             â”‚
â”‚ â€¢ Server: http://localhost:8000                                            â”‚
â”‚ â€¢ Model: microsoft/Phi-3-mini-4k-instruct                                  â”‚
â”‚                                                                            â”‚
â”‚ Commands: /help, /clear, /history, /quit                                   â”‚
â”‚ Type your message and press Enter to chat!                                 â”‚
â”‚                                                                            â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

You: Hello! How are you today?
â•­â”€ ğŸ¤– Assistant â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Hello! I'm doing well, thank you for asking. I'm here and ready to help   â”‚
â”‚ you with any questions or tasks you might have. How can I assist you      â”‚
â”‚ today?                                                                     â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

You: Tell me about machine learning
â•­â”€ ğŸ¤– Assistant â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Machine learning is a fascinating field that involves teaching computers  â”‚
â”‚ to learn from data and make predictions or decisions without being        â”‚
â”‚ explicitly programmed for each specific task. It's a subset of artificial â”‚
â”‚ intelligence that focuses on algorithms and statistical models that can   â”‚
â”‚ improve their performance as they are exposed to more data.               â”‚
â”‚                                                                            â”‚
â”‚ There are several main types of machine learning:                          â”‚
â”‚                                                                            â”‚
â”‚ 1. **Supervised Learning**: The algorithm learns from labeled training    â”‚
â”‚    data to make predictions on new, unseen data. Examples include          â”‚
â”‚    classification (like spam detection) and regression (like predicting    â”‚
â”‚    house prices).                                                          â”‚
â”‚                                                                            â”‚
â”‚ 2. **Unsupervised Learning**: The algorithm finds patterns in data        â”‚
â”‚    without labeled examples. This includes clustering (grouping similar    â”‚
â”‚    data points) and dimensionality reduction.                              â”‚
â”‚                                                                            â”‚
â”‚ 3. **Reinforcement Learning**: An agent learns through trial and error by â”‚
â”‚    interacting with an environment, receiving rewards or penalties for     â”‚
â”‚    actions.                                                                â”‚
â”‚                                                                            â”‚
â”‚ Machine learning has applications in many fields including computer        â”‚
â”‚ vision, natural language processing, recommendation systems, autonomous    â”‚
â”‚ vehicles, medical diagnosis, and financial trading.                        â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

You: /history
â•­â”€ ğŸ“ Conversation History â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“ â”‚
â”‚ â”ƒ Turn â”ƒ Role         â”ƒ Content                                         â”ƒ â”‚
â”‚ â”¡â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”© â”‚
â”‚ â”‚ 1    â”‚ User         â”‚ Hello! How are you today?                       â”‚
â”‚ â”‚ 2    â”‚ Assistant    â”‚ Hello! I'm doing well, thank you for asking. ... â”‚
â”‚ â”‚ 3    â”‚ User         â”‚ Tell me about machine learning                  â”‚
â”‚ â”‚ 4    â”‚ Assistant    â”‚ Machine learning is a fascinating field that... â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

You: /quit
ğŸ‘‹ Goodbye!
```

## Enhanced Chat Client with MCP Tools

The `mixvllm-chat` command provides an advanced chat client with MCP (Model Context Protocol) tool integration, enabling the LLM to call external tools during conversations.

### Features

- **MCP Tool Integration**: Weather queries and other MCP tools
- **Tool Discovery Display**: Shows available MCP tools on startup
- **Dual Modes**: Simple chat or agent mode with tool calling
- **Rich Terminal UI**: Enhanced formatting with panels and colors
- **Conversation Context**: Maintains chat history
- **Streaming Support**: Real-time response streaming
- **Command System**: `/help`, `/clear`, `/history`, `/mcp`, `/quit`

### Installation

Install additional dependencies for MCP support:

```bash
uv sync
```

### Usage

**Note**: Since vLLM serves only one model at a time, the `--model` parameter is optional. The client will automatically detect and use the model currently loaded on the server.

#### Simple Chat Mode (Default)

```bash
# Basic chat with vLLM server (auto-detects model)
uv run mixvllm-chat

# Connect to specific server (auto-detects model)
uv run mixvllm-chat --base-url http://localhost:8000

# Specify model explicitly (optional)
uv run mixvllm-chat --base-url http://localhost:8000 --model microsoft/Phi-3-mini-4k-instruct
```

#### MCP Agent Mode

```bash
# Enable MCP tools for weather queries (auto-detects model)
uv run mixvllm-chat --enable-mcp

# Full configuration with custom MCP config
uv run mixvllm-chat \
  --enable-mcp \
  --mcp-config configs/mcp_servers.yaml \
  --base-url http://localhost:8000 \
  --stream \
  --temperature 0.8
```

### MCP Tools Available

When MCP mode is enabled, the following tools are available:

- **Weather Queries**: Get current weather, forecasts, and historical data
- **Location Support**: Supports city names and coordinates
- **Units**: Celsius or Fahrenheit temperature units

### Example MCP Conversation

```
âœ“ Connected to vLLM server at http://localhost:8000
âœ“ Auto-selected model: microsoft/Phi-3-mini-4k-instruct
âœ“ MCP tools enabled (2 tools available)

â•­â”€ Welcome â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ ğŸ¤– Enhanced vLLM Chat Client (with MCP tools)                             â”‚
â”‚                                                                           â”‚
â”‚ Configuration:                                                            â”‚
â”‚ â€¢ Server: http://localhost:8000                                           â”‚
â”‚ â€¢ Model: microsoft/Phi-3-mini-4k-instruct                                 â”‚
â”‚ â€¢ MCP Tools: Enabled                                                      â”‚
â”‚                                                                           â”‚
â”‚ Available MCP Tools (2):                                                  â”‚
â”‚ â€¢ weather_get_hourly_weather - Get hourly weather forecast for a locationâ”‚
â”‚   using Open-Meteo API (Weather information and forecasts)               â”‚
â”‚ â€¢ weather_geocode_location - Get coordinates and timezone information forâ”‚
â”‚   a location. (Weather information and forecasts)                         â”‚
â”‚                                                                           â”‚
â”‚ Commands: /help, /clear, /history, /mcp, /quit                            â”‚
â”‚ Type your message and press Enter to chat!                                â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

You: What's the weather like in New York?
â•­â”€ ğŸŒ¤ï¸ Assistant (with tools) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ The user is asking about the weather in New York. I should use the        â”‚
â”‚ weather_get_weather tool to get current weather information.              â”‚
â”‚                                                                           â”‚
â”‚ Tool Call: weather_get_weather(location="New York", units="celsius")      â”‚
â”‚                                                                           â”‚
â”‚ Tool Result: [weather] Weather for New York: 22Â°C, Partly Cloudy, Wind 5  â”‚
â”‚ km/h                                                                     â”‚
â”‚                                                                           â”‚
â”‚ Current weather in New York: 22Â°C with partly cloudy conditions and light â”‚
â”‚ winds at 5 km/h.                                                          â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

You: /mcp
â•­â”€ ğŸ”§ MCP Integration Status â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”“ â”‚
â”‚ â”ƒ Server  â”ƒ Status                                        â”ƒ Tools       â”ƒ â”‚
â”‚ â”¡â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”© â”‚
â”‚ â”‚ weather â”‚ âœ“ Connected (2 tools)                         â”‚ get_hourly_ â”‚
â”‚ â”‚         â”‚                                               â”‚ weather,    â”‚
â”‚ â”‚         â”‚                                               â”‚ geocode_loc â”‚
â”‚ â”‚         â”‚                                               â”‚ ation       â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
```

