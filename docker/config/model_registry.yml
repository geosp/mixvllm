models:
  gpt-oss-20b:
    model: openai/gpt-oss-20b
    dtype: bfloat16
    tensor-parallel-size: 2
    gpu-memory-utilization: 0.35
    max-num-seqs: 8
    max-model-len: 131072
    description: Lightweight
  gpt-oss-120b:
    model: openai/gpt-oss-120b
    dtype: bfloat16
    tensor-parallel-size: 2
    gpu-memory-utilization: 0.55
    max-num-seqs: 8
    max-model-len: 131072
    description: Full-capacity
  llama-4:
    model: meta-llama/Llama-4-Scout-17B-16E-Instruct
    dtype: bfloat16
    tensor-parallel-size: 2
    gpu-memory-utilization: 0.75
    max-num-seqs: 8
    max-model-len: 131072
    description: "MoE multimodal long-context model (17B active, 16 experts)"
  deepseek-r1-70b:
    model: deepseek-ai/DeepSeek-R1-Distill-Llama-70B
    dtype: bfloat16
    tensor-parallel-size: 2
    gpu-memory-utilization: 0.60 
    max-num-seqs: 8
    max-model-len: 65536
    description: DeepSeek-R1-Distill-Llama-70B (reasoning-optimized)
  mixtral-8x7b:
    model: mistralai/Mixtral-8x7B-Instruct-v0.1
    dtype: bfloat16
    tensor-parallel-size: 2
    gpu-memory-utilization: 0.80
    max-num-seqs: 8 
    max-model-len: 32768
    description: Mixtral-8Ã—7B Instruct (MoE)

