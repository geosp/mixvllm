services:
  mixvllm-server:
    # Use pre-built image from GitHub Container Registry
    image: ghcr.io/geosp/mixvllm:latest

    container_name: mixvllm-server

    # GPU configuration
    # Docker will make all GPUs available to the container
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

    # Port mappings
    ports:
      - "8000:8000"   # Model server API
      - "8888:8888"   # Web terminal

    # Environment variables
    environment:
      - HF_TOKEN=${HF_TOKEN}
      - NVIDIA_VISIBLE_DEVICES=all

    # Volume for HuggingFace model cache
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
      
    # No need to override entrypoint since we changed the Dockerfile

    # Run mixvllm server script with improved logging
    command: >
      echo '==== Starting MixVLLM Container ====' &&
      echo 'Cloning mixvllm repository...' &&
      rm -rf /app/mixvllm-local || true &&
      git clone https://github.com/geosp/mixvllm.git /app/mixvllm-local &&
      cd /app/mixvllm-local &&
      echo 'Installing dependencies with uv...' &&
      uv pip install --system -e . &&
      echo 'Starting mixvllm server with GPU parallelism...' &&
      mixvllm-serve \
        --model gpt-oss-20b \
        --gpus 2 \
        --enable-terminal

    restart: unless-stopped
