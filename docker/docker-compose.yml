networks:
  mixvllm-net:
    name: mixvllm-net

services:
  # KNOWN LIMITATION: vLLM v0.11.0 (v1 engine) has issues with multi-GPU
  # tensor parallelism in Docker containers. Workers fail during initialization
  # due to IPC/shared memory communication issues. This works perfectly on the host.
  #
  # Workaround options:
  # 1. Use single GPU in Docker (this configuration)
  # 2. Run directly on host for multi-GPU support
  # 3. Wait for vLLM bug fix in future releases

  mixvllm-server:
    image: mixvllm:latest
    container_name: mixvllm-server

    entrypoint: ["/bin/sh", "-c"]
    command:
      - |
        launch --model gpt-oss-20b --terminal &
        wait

    ports:
      - "8000:8000"   # Model server API
      - "8888:8888"   # Web terminal

    volumes:
      - ~/.cache/huggingface:/home/mixvllm/.cache/huggingface

    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 2
              capabilities: [gpu]

    environment:
      - HF_TOKEN=${HF_TOKEN}
      # - NVIDIA_VISIBLE_DEVICES=0  # Use first GPU only
      - VLLM_LOGGING_LEVEL=INFO

    networks:
      - mixvllm-net

    restart: always
    
    # Note: This configuration:
    # 1. Copies all files from the repository to the container
    # 2. Fixes pyproject.toml by removing the README.md reference
    # 3. Installs the package in development mode with all dependencies
