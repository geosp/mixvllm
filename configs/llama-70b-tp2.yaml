# Configuration for Llama 2 70B model with tensor parallelism
# Large model requiring both GPUs

model:
  name: "meta-llama/Llama-2-70b-hf"
  trust_remote_code: true

inference:
  tensor_parallel_size: 2
  gpu_memory_utilization: 0.9
  max_model_len: 4096

server:
  host: "0.0.0.0"
  port: 8002

generation_defaults:
  temperature: 0.7
  max_tokens: 512